{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tifffile\n",
      "  Downloading tifffile-2020.8.13-py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tifffile) (1.18.1)\n",
      "Installing collected packages: tifffile\n",
      "Successfully installed tifffile-2020.8.13\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting imagecodecs\n",
      "  Downloading imagecodecs-2020.5.30-cp36-cp36m-manylinux2014_x86_64.whl (17.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.9 MB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from imagecodecs) (1.18.1)\n",
      "Installing collected packages: imagecodecs\n",
      "Successfully installed imagecodecs-2020.5.30\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting torch\n",
      "  Downloading torch-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (748.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 748.8 MB 4.3 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: future in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torch) (1.18.1)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.6.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (5.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9 MB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torchvision) (1.18.1)\n",
      "Requirement already satisfied: torch==1.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torchvision) (1.6.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torchvision) (7.0.0)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torch==1.6.0->torchvision) (0.18.2)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.7.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting efficientnet_pytorch\n",
      "  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\n",
      "Collecting torchtoolbox\n",
      "  Downloading torchtoolbox-0.1.5-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 3.1 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from efficientnet_pytorch) (1.6.0)\n",
      "Requirement already satisfied: opencv-python in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torchtoolbox) (4.2.0.32)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torchtoolbox) (4.44.1)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-1.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.2 MB 9.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting lmdb\n",
      "  Downloading lmdb-0.99.tar.gz (995 kB)\n",
      "\u001b[K     |████████████████████████████████| 995 kB 41.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torchtoolbox) (1.18.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torchtoolbox) (1.14.0)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torchtoolbox) (0.22.1)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torchtoolbox) (1.4.1)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torch->efficientnet_pytorch) (0.18.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit-learn->torchtoolbox) (0.14.1)\n",
      "Building wheels for collected packages: efficientnet-pytorch, lmdb\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12419 sha256=5cc7b9ffa1a5516b643a7a2a4ef66f3a2662f8ed98910062ecac96982e5e724f\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/1c/07/d1/ff77968842daae1dde944173a8e8a7be193646d37842f13b24\n",
      "  Building wheel for lmdb (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lmdb: filename=lmdb-0.99-cp36-cp36m-linux_x86_64.whl size=263396 sha256=32585dbb5d14a1fd04458ae00f8e6edb1aaf8a53b3221fc0a69900ede23ea77f\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/e3/3b/8c/cf3bfd596f6682660a06ad16faa7547ed392a42a6582f62317\n",
      "Successfully built efficientnet-pytorch lmdb\n",
      "Installing collected packages: efficientnet-pytorch, pyarrow, lmdb, torchtoolbox\n",
      "Successfully installed efficientnet-pytorch-0.6.3 lmdb-0.99 pyarrow-1.0.0 torchtoolbox-0.1.5\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting scikit-build\n",
      "  Downloading scikit_build-0.11.1-py2.py3-none-any.whl (72 kB)\n",
      "\u001b[K     |████████████████████████████████| 72 kB 111 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=28.0.0; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit-build) (46.1.3.post20200330)\n",
      "Requirement already satisfied: wheel>=0.29.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit-build) (0.34.2)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit-build) (20.3)\n",
      "Requirement already satisfied: distro in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit-build) (1.5.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging->scikit-build) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging->scikit-build) (2.4.6)\n",
      "Installing collected packages: scikit-build\n",
      "Successfully installed scikit-build-0.11.1\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting blosc\n",
      "  Downloading blosc-1.9.1.tar.gz (809 kB)\n",
      "\u001b[K     |████████████████████████████████| 809 kB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /home/ec2-user/anaconda3/envs/python3/bin/python /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-naq65p7d/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools wheel scikit-build cmake ninja\n",
      "       cwd: None\n",
      "  Complete output (14 lines):\n",
      "  Traceback (most recent call last):\n",
      "    File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "      \"__main__\", mod_spec)\n",
      "    File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "      exec(code, run_globals)\n",
      "    File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pip/__main__.py\", line 16, in <module>\n",
      "      from pip._internal.cli.main import main as _main  # isort:skip # noqa\n",
      "    File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pip/_internal/cli/main.py\", line 5, in <module>\n",
      "      import locale\n",
      "    File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/locale.py\", line 16, in <module>\n",
      "      import re\n",
      "    File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.6/re.py\", line 142, in <module>\n",
      "      class RegexFlag(enum.IntFlag):\n",
      "  AttributeError: module 'enum' has no attribute 'IntFlag'\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: /home/ec2-user/anaconda3/envs/python3/bin/python /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-naq65p7d/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools wheel scikit-build cmake ninja Check the logs for full command output.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Don't need to run this each time notebook is started \n",
    "# Reqs already satisfied in kaggle NB.\n",
    "!pip install tifffile \n",
    "!pip install imagecodecs\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install efficientnet_pytorch torchtoolbox\n",
    "!pip install scikit-build\n",
    "!pip install blosc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Imports here\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import skimage.io\n",
    "#from csv_loader import load_csv\n",
    "\n",
    "# Tiff visualisation imports and downloads\n",
    "import numpy as np\n",
    "#import tifffile as tiff\n",
    "\n",
    "# For re-importing python modules\n",
    "import importlib\n",
    "#importlib.reload(csv_loader.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>data_provider</th>\n",
       "      <th>isup_grade</th>\n",
       "      <th>gleason_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0005f7aaab2800f6170c399693a96917</td>\n",
       "      <td>karolinska</td>\n",
       "      <td>0</td>\n",
       "      <td>0+0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000920ad0b612851f8e01bcc880d9b3d</td>\n",
       "      <td>karolinska</td>\n",
       "      <td>0</td>\n",
       "      <td>0+0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0018ae58b01bdadc8e347995b69f99aa</td>\n",
       "      <td>radboud</td>\n",
       "      <td>4</td>\n",
       "      <td>4+4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001c62abd11fa4b57bf7a6c603a11bb9</td>\n",
       "      <td>karolinska</td>\n",
       "      <td>4</td>\n",
       "      <td>4+4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001d865e65ef5d2579c190a0e0350d8f</td>\n",
       "      <td>karolinska</td>\n",
       "      <td>0</td>\n",
       "      <td>0+0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10611</th>\n",
       "      <td>ffd2841373b39792ab0c84cccd066e31</td>\n",
       "      <td>radboud</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10612</th>\n",
       "      <td>ffdc59cd580a1468eac0e6a32dd1ff2d</td>\n",
       "      <td>radboud</td>\n",
       "      <td>5</td>\n",
       "      <td>4+5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10613</th>\n",
       "      <td>ffe06afd66a93258f8fabdef6044e181</td>\n",
       "      <td>radboud</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10614</th>\n",
       "      <td>ffe236a25d4cbed59438220799920749</td>\n",
       "      <td>radboud</td>\n",
       "      <td>2</td>\n",
       "      <td>3+4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10615</th>\n",
       "      <td>ffe9bcababc858e04840669e788065a1</td>\n",
       "      <td>radboud</td>\n",
       "      <td>4</td>\n",
       "      <td>4+4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10616 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               image_id data_provider  isup_grade  \\\n",
       "0      0005f7aaab2800f6170c399693a96917    karolinska           0   \n",
       "1      000920ad0b612851f8e01bcc880d9b3d    karolinska           0   \n",
       "2      0018ae58b01bdadc8e347995b69f99aa       radboud           4   \n",
       "3      001c62abd11fa4b57bf7a6c603a11bb9    karolinska           4   \n",
       "4      001d865e65ef5d2579c190a0e0350d8f    karolinska           0   \n",
       "...                                 ...           ...         ...   \n",
       "10611  ffd2841373b39792ab0c84cccd066e31       radboud           0   \n",
       "10612  ffdc59cd580a1468eac0e6a32dd1ff2d       radboud           5   \n",
       "10613  ffe06afd66a93258f8fabdef6044e181       radboud           0   \n",
       "10614  ffe236a25d4cbed59438220799920749       radboud           2   \n",
       "10615  ffe9bcababc858e04840669e788065a1       radboud           4   \n",
       "\n",
       "      gleason_score  \n",
       "0               0+0  \n",
       "1               0+0  \n",
       "2               4+4  \n",
       "3               4+4  \n",
       "4               0+0  \n",
       "...             ...  \n",
       "10611      negative  \n",
       "10612           4+5  \n",
       "10613      negative  \n",
       "10614           3+4  \n",
       "10615           4+4  \n",
       "\n",
       "[10616 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class load_csv(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)# todo remove sample for debug\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0] +\".tiff\")\n",
    "        image = torch.from_numpy(skimage.io.MultiImage(img_path)[2]).permute(2,0,1).float()\n",
    "        #Image.MAX_IMAGE_PIXELS = None\n",
    "                \n",
    "        '''\n",
    "        train_transform = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                              transforms.RandomResizedCrop(224),\n",
    "                                              transforms.ToPILImage(),\n",
    "                                              transforms.RandomHorizontalFlip(),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                                   [0.229, 0.224, 0.225])])\n",
    "        '''\n",
    "        #image.transform = transforms.RandomResizedCrop(224)\n",
    "        \n",
    "        isup_grade = int(self.annotations.iloc[index,:]['isup_grade'])\n",
    "        #label = np.zeros(5).astype(np.float32)\n",
    "        #y_label = label[:isup_grade] = 1.\n",
    "        y_label = torch.tensor(isup_grade)\n",
    "        \n",
    "        self.transform= transforms.Compose([transforms.ToPILImage(),\n",
    "                                            transforms.RandomResizedCrop(400), \n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                                   [0.229, 0.224, 0.225])])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return (image, y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_info = pd.read_csv('train.csv').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10000\n",
    "first_train_list = file_info.sample(sample_size)\n",
    "first_train_list \n",
    "first_train_list.to_csv(\"sample_dir/sample.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = load_csv(csv_file='sample_dir/sample.csv', root_dir='train_images', transform=transforms.ToTensor())\n",
    "dataset = load_csv(csv_file='sample_dir/sample.csv', root_dir='train_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a subset of 50 sample images to work with will be good to start building a first neural network and debug .TIFF related issues\n",
    "sample_size = dataset.annotations.shape[0]\n",
    "train_ratio = .75\n",
    "valid_ratio = .15\n",
    "test_ratio = 1-(train_ratio + valid_ratio)\n",
    "train_size = int(train_ratio*sample_size)\n",
    "valid_size = int(valid_ratio*sample_size)\n",
    "test_size = int(test_ratio*sample_size)\n",
    "\n",
    "# In case there are decimal/rounding errors\n",
    "if train_size + valid_size + test_size > sample_size:\n",
    "    train_size-=1\n",
    "elif train_size + valid_size + test_size < sample_size:\n",
    "    train_size+=1\n",
    "\n",
    "# List of sample images with corresponding labels\n",
    "#image_id = random.sample(list(file_info['image_id']), sample_size)\n",
    "#y_label = [int(file_info.loc[file_info['image_id'] == x, 'isup_grade']) for x in image_id]\n",
    "#image_id = [str(x) + '.tiff' for x in image_id]\n",
    "#sample_imgs = pd.DataFrame({'image_id':image_id, 'y_label':y_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7501, 1500, 999]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[train_size, valid_size, test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set, test_set = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass transforms in here, then run the next cell to see how the transforms look\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=10, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=10, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b1\n"
     ]
    }
   ],
   "source": [
    "# Define model type and freeze parameters\n",
    "#model = models.vgg16(pretrained=True)\n",
    "# for efficientnet self.arch._fc = nn.Linear(in_features=1280, out_features=500, bias=True)\n",
    "model = EfficientNet.from_pretrained('efficientnet-b1',num_classes=6)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "num_ftrs = model._fc.in_features\n",
    "    \n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True    \n",
    "#for param in model.parameters():\n",
    "#    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Squential Classifier transformation with high drop out rate\n",
    "# classifier = nn.Sequential(\n",
    "#     nn.Linear(in_features=1280, out_features=500, bias=True),\n",
    "#     #nn.Linear(224, 112, bias=True),\n",
    "#                            nn.ReLU(),\n",
    "#                            nn.Dropout(.5),\n",
    "# #                            nn.Linear(112, 66, bias=True),\n",
    "# #                            nn.ReLU(),\n",
    "# #                            nn.Dropout(.5),\n",
    "#                            nn.Linear(112, 1, bias=True))\n",
    "    \n",
    "# model.classifier = classifier\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 3, 3])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 1, 3, 3])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([8, 32, 1, 1])\n",
      "torch.Size([8])\n",
      "torch.Size([32, 8, 1, 1])\n",
      "torch.Size([32])\n",
      "torch.Size([16, 32, 1, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 1, 3, 3])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([4, 16, 1, 1])\n",
      "torch.Size([4])\n",
      "torch.Size([16, 4, 1, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 16, 1, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([96, 16, 1, 1])\n",
      "torch.Size([96])\n",
      "torch.Size([96])\n",
      "torch.Size([96, 1, 3, 3])\n",
      "torch.Size([96])\n",
      "torch.Size([96])\n",
      "torch.Size([4, 96, 1, 1])\n",
      "torch.Size([4])\n",
      "torch.Size([96, 4, 1, 1])\n",
      "torch.Size([96])\n",
      "torch.Size([24, 96, 1, 1])\n",
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([144, 24, 1, 1])\n",
      "torch.Size([144])\n",
      "torch.Size([144])\n",
      "torch.Size([144, 1, 3, 3])\n",
      "torch.Size([144])\n",
      "torch.Size([144])\n",
      "torch.Size([6, 144, 1, 1])\n",
      "torch.Size([6])\n",
      "torch.Size([144, 6, 1, 1])\n",
      "torch.Size([144])\n",
      "torch.Size([24, 144, 1, 1])\n",
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([144, 24, 1, 1])\n",
      "torch.Size([144])\n",
      "torch.Size([144])\n",
      "torch.Size([144, 1, 3, 3])\n",
      "torch.Size([144])\n",
      "torch.Size([144])\n",
      "torch.Size([6, 144, 1, 1])\n",
      "torch.Size([6])\n",
      "torch.Size([144, 6, 1, 1])\n",
      "torch.Size([144])\n",
      "torch.Size([24, 144, 1, 1])\n",
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([144, 24, 1, 1])\n",
      "torch.Size([144])\n",
      "torch.Size([144])\n",
      "torch.Size([144, 1, 5, 5])\n",
      "torch.Size([144])\n",
      "torch.Size([144])\n",
      "torch.Size([6, 144, 1, 1])\n",
      "torch.Size([6])\n",
      "torch.Size([144, 6, 1, 1])\n",
      "torch.Size([144])\n",
      "torch.Size([40, 144, 1, 1])\n",
      "torch.Size([40])\n",
      "torch.Size([40])\n",
      "torch.Size([240, 40, 1, 1])\n",
      "torch.Size([240])\n",
      "torch.Size([240])\n",
      "torch.Size([240, 1, 5, 5])\n",
      "torch.Size([240])\n",
      "torch.Size([240])\n",
      "torch.Size([10, 240, 1, 1])\n",
      "torch.Size([10])\n",
      "torch.Size([240, 10, 1, 1])\n",
      "torch.Size([240])\n",
      "torch.Size([40, 240, 1, 1])\n",
      "torch.Size([40])\n",
      "torch.Size([40])\n",
      "torch.Size([240, 40, 1, 1])\n",
      "torch.Size([240])\n",
      "torch.Size([240])\n",
      "torch.Size([240, 1, 5, 5])\n",
      "torch.Size([240])\n",
      "torch.Size([240])\n",
      "torch.Size([10, 240, 1, 1])\n",
      "torch.Size([10])\n",
      "torch.Size([240, 10, 1, 1])\n",
      "torch.Size([240])\n",
      "torch.Size([40, 240, 1, 1])\n",
      "torch.Size([40])\n",
      "torch.Size([40])\n",
      "torch.Size([240, 40, 1, 1])\n",
      "torch.Size([240])\n",
      "torch.Size([240])\n",
      "torch.Size([240, 1, 3, 3])\n",
      "torch.Size([240])\n",
      "torch.Size([240])\n",
      "torch.Size([10, 240, 1, 1])\n",
      "torch.Size([10])\n",
      "torch.Size([240, 10, 1, 1])\n",
      "torch.Size([240])\n",
      "torch.Size([80, 240, 1, 1])\n",
      "torch.Size([80])\n",
      "torch.Size([80])\n",
      "torch.Size([480, 80, 1, 1])\n",
      "torch.Size([480])\n",
      "torch.Size([480])\n",
      "torch.Size([480, 1, 3, 3])\n",
      "torch.Size([480])\n",
      "torch.Size([480])\n",
      "torch.Size([20, 480, 1, 1])\n",
      "torch.Size([20])\n",
      "torch.Size([480, 20, 1, 1])\n",
      "torch.Size([480])\n",
      "torch.Size([80, 480, 1, 1])\n",
      "torch.Size([80])\n",
      "torch.Size([80])\n",
      "torch.Size([480, 80, 1, 1])\n",
      "torch.Size([480])\n",
      "torch.Size([480])\n",
      "torch.Size([480, 1, 3, 3])\n",
      "torch.Size([480])\n",
      "torch.Size([480])\n",
      "torch.Size([20, 480, 1, 1])\n",
      "torch.Size([20])\n",
      "torch.Size([480, 20, 1, 1])\n",
      "torch.Size([480])\n",
      "torch.Size([80, 480, 1, 1])\n",
      "torch.Size([80])\n",
      "torch.Size([80])\n",
      "torch.Size([480, 80, 1, 1])\n",
      "torch.Size([480])\n",
      "torch.Size([480])\n",
      "torch.Size([480, 1, 3, 3])\n",
      "torch.Size([480])\n",
      "torch.Size([480])\n",
      "torch.Size([20, 480, 1, 1])\n",
      "torch.Size([20])\n",
      "torch.Size([480, 20, 1, 1])\n",
      "torch.Size([480])\n",
      "torch.Size([80, 480, 1, 1])\n",
      "torch.Size([80])\n",
      "torch.Size([80])\n",
      "torch.Size([480, 80, 1, 1])\n",
      "torch.Size([480])\n",
      "torch.Size([480])\n",
      "torch.Size([480, 1, 5, 5])\n",
      "torch.Size([480])\n",
      "torch.Size([480])\n",
      "torch.Size([20, 480, 1, 1])\n",
      "torch.Size([20])\n",
      "torch.Size([480, 20, 1, 1])\n",
      "torch.Size([480])\n",
      "torch.Size([112, 480, 1, 1])\n",
      "torch.Size([112])\n",
      "torch.Size([112])\n",
      "torch.Size([672, 112, 1, 1])\n",
      "torch.Size([672])\n",
      "torch.Size([672])\n",
      "torch.Size([672, 1, 5, 5])\n",
      "torch.Size([672])\n",
      "torch.Size([672])\n",
      "torch.Size([28, 672, 1, 1])\n",
      "torch.Size([28])\n",
      "torch.Size([672, 28, 1, 1])\n",
      "torch.Size([672])\n",
      "torch.Size([112, 672, 1, 1])\n",
      "torch.Size([112])\n",
      "torch.Size([112])\n",
      "torch.Size([672, 112, 1, 1])\n",
      "torch.Size([672])\n",
      "torch.Size([672])\n",
      "torch.Size([672, 1, 5, 5])\n",
      "torch.Size([672])\n",
      "torch.Size([672])\n",
      "torch.Size([28, 672, 1, 1])\n",
      "torch.Size([28])\n",
      "torch.Size([672, 28, 1, 1])\n",
      "torch.Size([672])\n",
      "torch.Size([112, 672, 1, 1])\n",
      "torch.Size([112])\n",
      "torch.Size([112])\n",
      "torch.Size([672, 112, 1, 1])\n",
      "torch.Size([672])\n",
      "torch.Size([672])\n",
      "torch.Size([672, 1, 5, 5])\n",
      "torch.Size([672])\n",
      "torch.Size([672])\n",
      "torch.Size([28, 672, 1, 1])\n",
      "torch.Size([28])\n",
      "torch.Size([672, 28, 1, 1])\n",
      "torch.Size([672])\n",
      "torch.Size([112, 672, 1, 1])\n",
      "torch.Size([112])\n",
      "torch.Size([112])\n",
      "torch.Size([672, 112, 1, 1])\n",
      "torch.Size([672])\n",
      "torch.Size([672])\n",
      "torch.Size([672, 1, 5, 5])\n",
      "torch.Size([672])\n",
      "torch.Size([672])\n",
      "torch.Size([28, 672, 1, 1])\n",
      "torch.Size([28])\n",
      "torch.Size([672, 28, 1, 1])\n",
      "torch.Size([672])\n",
      "torch.Size([192, 672, 1, 1])\n",
      "torch.Size([192])\n",
      "torch.Size([192])\n",
      "torch.Size([1152, 192, 1, 1])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1, 5, 5])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([48, 1152, 1, 1])\n",
      "torch.Size([48])\n",
      "torch.Size([1152, 48, 1, 1])\n",
      "torch.Size([1152])\n",
      "torch.Size([192, 1152, 1, 1])\n",
      "torch.Size([192])\n",
      "torch.Size([192])\n",
      "torch.Size([1152, 192, 1, 1])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1, 5, 5])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([48, 1152, 1, 1])\n",
      "torch.Size([48])\n",
      "torch.Size([1152, 48, 1, 1])\n",
      "torch.Size([1152])\n",
      "torch.Size([192, 1152, 1, 1])\n",
      "torch.Size([192])\n",
      "torch.Size([192])\n",
      "torch.Size([1152, 192, 1, 1])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1, 5, 5])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([48, 1152, 1, 1])\n",
      "torch.Size([48])\n",
      "torch.Size([1152, 48, 1, 1])\n",
      "torch.Size([1152])\n",
      "torch.Size([192, 1152, 1, 1])\n",
      "torch.Size([192])\n",
      "torch.Size([192])\n",
      "torch.Size([1152, 192, 1, 1])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1, 5, 5])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([48, 1152, 1, 1])\n",
      "torch.Size([48])\n",
      "torch.Size([1152, 48, 1, 1])\n",
      "torch.Size([1152])\n",
      "torch.Size([192, 1152, 1, 1])\n",
      "torch.Size([192])\n",
      "torch.Size([192])\n",
      "torch.Size([1152, 192, 1, 1])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152, 1, 3, 3])\n",
      "torch.Size([1152])\n",
      "torch.Size([1152])\n",
      "torch.Size([48, 1152, 1, 1])\n",
      "torch.Size([48])\n",
      "torch.Size([1152, 48, 1, 1])\n",
      "torch.Size([1152])\n",
      "torch.Size([320, 1152, 1, 1])\n",
      "torch.Size([320])\n",
      "torch.Size([320])\n",
      "torch.Size([1920, 320, 1, 1])\n",
      "torch.Size([1920])\n",
      "torch.Size([1920])\n",
      "torch.Size([1920, 1, 3, 3])\n",
      "torch.Size([1920])\n",
      "torch.Size([1920])\n",
      "torch.Size([80, 1920, 1, 1])\n",
      "torch.Size([80])\n",
      "torch.Size([1920, 80, 1, 1])\n",
      "torch.Size([1920])\n",
      "torch.Size([320, 1920, 1, 1])\n",
      "torch.Size([320])\n",
      "torch.Size([320])\n",
      "torch.Size([1280, 320, 1, 1])\n",
      "torch.Size([1280])\n",
      "torch.Size([1280])\n",
      "torch.Size([6, 1280])\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.NLLLoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "#criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 10\n",
    "print_every = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data_function(model, test_loader, criterion):\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    \n",
    "    for ii, (inputs, labels) in enumerate(test_loader):\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        output = model.forward(inputs)\n",
    "        test_loss += criterion(output, labels.long()).item()\n",
    "        \n",
    "        ps = torch.exp(output)\n",
    "        equality = (labels == ps.argmax(dim=1))\n",
    "        accuracy += equality.type(torch.FloatTensor).mean()\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_test(epochs, model, train_loader, device, optimizer, criterion, print_every, valid_loader):\n",
    "    start = time.time()\n",
    "\n",
    "    test_loss = 0\n",
    "    steps = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        \n",
    "        for ii, (inputs, labels) in enumerate(train_loader):\n",
    "            steps += 1\n",
    "        \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "       \n",
    "            outputs = model.forward(inputs)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            if steps % print_every == 0:\n",
    "                model.eval()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    valid_loss, accuracy = validate_data_function(model, valid_loader, criterion)\n",
    "            \n",
    "                print(f\"Epoch {epoch+1}/{epochs}..| \"\n",
    "                      f\"Train loss: {running_loss/print_every:.3f}..| \"\n",
    "                      f\"Validation loss: {valid_loss/print_every:.3f}..| \"                  \n",
    "                      f\"Validation accuracy: {accuracy/len(valid_loader):.3f}|\")\n",
    "            \n",
    "                running_loss = 0\n",
    "                model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I've built and ran a first version of this NN. Now, as I go about making my first adjustments, I have several decisions I need to make:\n",
    "Question 1) Which of the three picture qualities do I want to choose from my multi-dimensional .TIFF file?\n",
    "Preliminary answer/thoughts: Not the highest resolution (dim = 0) because it overloads the kernel's memory (some of these pictures can reach 500MB in size). It would have to be either medium (dim = 1) or low (dim = 2) quality pictures.\n",
    "\n",
    "Question 2) What should the size of my input be when I feed it into the model?\n",
    "Preliminary answer/thoughts: This will depend on the answer to Question 1. It will also depend on pre-processing transformations that could be needed to improve prediction scores.\n",
    "\n",
    "Question 3) Should I train my model with more than 1 folds?\n",
    "Preliminary answer/thoughts: Yes. The model must be evenly trained by the same training data because, according to Kaggle competition, the model will be evaluated training-data-based testing metric (read project proposal.pdf in project repository).\n",
    "\n",
    "These questions are designed to promote greater prediction scores while being conscious of training times and system capacities. Further below, I will be recording parameters and results for different tests, and hope to eventually have answers for the questions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tile approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10..| Train loss: -25.058..| Validation loss: -1.850..| Validation accuracy: 0.067|\n",
      "Epoch 2/10..| Train loss: -25.208..| Validation loss: -8.110..| Validation accuracy: 0.000|\n",
      "Epoch 3/10..| Train loss: -31.776..| Validation loss: -8.674..| Validation accuracy: 0.067|\n",
      "Epoch 4/10..| Train loss: -34.874..| Validation loss: -10.048..| Validation accuracy: 0.000|\n",
      "Epoch 5/10..| Train loss: -36.871..| Validation loss: -13.750..| Validation accuracy: 0.200|\n",
      "Epoch 6/10..| Train loss: -26.755..| Validation loss: -12.898..| Validation accuracy: 0.067|\n",
      "Epoch 7/10..| Train loss: -46.194..| Validation loss: -5.996..| Validation accuracy: 0.267|\n",
      "Epoch 8/10..| Train loss: -51.002..| Validation loss: -4.397..| Validation accuracy: 0.267|\n",
      "Epoch 9/10..| Train loss: -47.457..| Validation loss: -8.642..| Validation accuracy: 0.133|\n",
      "Epoch 10/10..| Train loss: -44.559..| Validation loss: -12.302..| Validation accuracy: 0.067|\n",
      "51.41079807281494\n"
     ]
    }
   ],
   "source": [
    "# 1000 sample pics, low quality, 224 randomcrop frame size\n",
    "train_test(epochs, model, train_loader, device, optimizer, criterion, print_every, valid_loader)\n",
    "# Result: 51.4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10..| Train loss: 1.734..| Validation loss: 1.818..| Validation accuracy: 0.240|\n",
      "Epoch 1/10..| Train loss: 1.826..| Validation loss: 1.832..| Validation accuracy: 0.187|\n",
      "Epoch 1/10..| Train loss: 1.706..| Validation loss: 1.720..| Validation accuracy: 0.280|\n",
      "Epoch 1/10..| Train loss: 1.789..| Validation loss: 1.723..| Validation accuracy: 0.293|\n",
      "Epoch 1/10..| Train loss: 1.665..| Validation loss: 1.730..| Validation accuracy: 0.240|\n",
      "Epoch 2/10..| Train loss: 1.245..| Validation loss: 1.685..| Validation accuracy: 0.253|\n",
      "Epoch 2/10..| Train loss: 1.718..| Validation loss: 1.676..| Validation accuracy: 0.253|\n",
      "Epoch 2/10..| Train loss: 1.705..| Validation loss: 1.707..| Validation accuracy: 0.213|\n",
      "Epoch 2/10..| Train loss: 1.658..| Validation loss: 1.679..| Validation accuracy: 0.267|\n",
      "Epoch 2/10..| Train loss: 1.429..| Validation loss: 1.944..| Validation accuracy: 0.320|\n",
      "Epoch 3/10..| Train loss: 0.819..| Validation loss: 3.428..| Validation accuracy: 0.320|\n",
      "Epoch 3/10..| Train loss: 1.603..| Validation loss: 3.176..| Validation accuracy: 0.333|\n",
      "Epoch 3/10..| Train loss: 1.598..| Validation loss: 2.812..| Validation accuracy: 0.347|\n",
      "Epoch 3/10..| Train loss: 1.549..| Validation loss: 3.114..| Validation accuracy: 0.347|\n",
      "Epoch 3/10..| Train loss: 1.738..| Validation loss: 3.032..| Validation accuracy: 0.333|\n",
      "Epoch 4/10..| Train loss: 0.658..| Validation loss: 2.345..| Validation accuracy: 0.280|\n",
      "Epoch 4/10..| Train loss: 1.505..| Validation loss: 2.180..| Validation accuracy: 0.240|\n",
      "Epoch 4/10..| Train loss: 1.619..| Validation loss: 2.046..| Validation accuracy: 0.253|\n",
      "Epoch 4/10..| Train loss: 1.488..| Validation loss: 2.037..| Validation accuracy: 0.307|\n",
      "Epoch 4/10..| Train loss: 1.519..| Validation loss: 2.047..| Validation accuracy: 0.293|\n",
      "Epoch 5/10..| Train loss: 0.352..| Validation loss: 1.976..| Validation accuracy: 0.307|\n",
      "Epoch 5/10..| Train loss: 1.424..| Validation loss: 2.004..| Validation accuracy: 0.187|\n",
      "Epoch 5/10..| Train loss: 1.394..| Validation loss: 2.141..| Validation accuracy: 0.213|\n",
      "Epoch 5/10..| Train loss: 1.594..| Validation loss: 1.724..| Validation accuracy: 0.320|\n",
      "Epoch 5/10..| Train loss: 1.359..| Validation loss: 2.108..| Validation accuracy: 0.293|\n",
      "Epoch 5/10..| Train loss: 1.567..| Validation loss: 1.961..| Validation accuracy: 0.227|\n",
      "Epoch 6/10..| Train loss: 1.493..| Validation loss: 2.167..| Validation accuracy: 0.280|\n",
      "Epoch 6/10..| Train loss: 1.392..| Validation loss: 2.242..| Validation accuracy: 0.173|\n",
      "Epoch 6/10..| Train loss: 1.659..| Validation loss: 1.926..| Validation accuracy: 0.280|\n",
      "Epoch 6/10..| Train loss: 1.411..| Validation loss: 1.933..| Validation accuracy: 0.293|\n",
      "Epoch 6/10..| Train loss: 1.392..| Validation loss: 1.717..| Validation accuracy: 0.360|\n",
      "Epoch 7/10..| Train loss: 1.115..| Validation loss: 1.899..| Validation accuracy: 0.373|\n",
      "Epoch 7/10..| Train loss: 1.310..| Validation loss: 2.196..| Validation accuracy: 0.307|\n",
      "Epoch 7/10..| Train loss: 1.384..| Validation loss: 2.847..| Validation accuracy: 0.173|\n",
      "Epoch 7/10..| Train loss: 1.644..| Validation loss: 2.734..| Validation accuracy: 0.240|\n",
      "Epoch 7/10..| Train loss: 1.309..| Validation loss: 2.042..| Validation accuracy: 0.360|\n",
      "Epoch 8/10..| Train loss: 0.875..| Validation loss: 2.517..| Validation accuracy: 0.347|\n",
      "Epoch 8/10..| Train loss: 1.298..| Validation loss: 3.628..| Validation accuracy: 0.387|\n",
      "Epoch 8/10..| Train loss: 1.366..| Validation loss: 2.569..| Validation accuracy: 0.333|\n",
      "Epoch 8/10..| Train loss: 1.385..| Validation loss: 2.180..| Validation accuracy: 0.293|\n",
      "Epoch 8/10..| Train loss: 1.585..| Validation loss: 1.723..| Validation accuracy: 0.347|\n",
      "Epoch 9/10..| Train loss: 0.518..| Validation loss: 1.693..| Validation accuracy: 0.387|\n",
      "Epoch 9/10..| Train loss: 1.302..| Validation loss: 1.574..| Validation accuracy: 0.360|\n",
      "Epoch 9/10..| Train loss: 1.329..| Validation loss: 1.613..| Validation accuracy: 0.467|\n",
      "Epoch 9/10..| Train loss: 1.583..| Validation loss: 1.605..| Validation accuracy: 0.427|\n",
      "Epoch 9/10..| Train loss: 1.414..| Validation loss: 1.747..| Validation accuracy: 0.347|\n",
      "Epoch 10/10..| Train loss: 0.290..| Validation loss: 1.776..| Validation accuracy: 0.373|\n",
      "Epoch 10/10..| Train loss: 1.330..| Validation loss: 1.960..| Validation accuracy: 0.360|\n",
      "Epoch 10/10..| Train loss: 1.360..| Validation loss: 1.873..| Validation accuracy: 0.280|\n",
      "Epoch 10/10..| Train loss: 1.377..| Validation loss: 1.954..| Validation accuracy: 0.320|\n",
      "Epoch 10/10..| Train loss: 1.314..| Validation loss: 1.723..| Validation accuracy: 0.280|\n",
      "Epoch 10/10..| Train loss: 1.595..| Validation loss: 1.681..| Validation accuracy: 0.280|\n",
      "5290.915365457535\n"
     ]
    }
   ],
   "source": [
    "# New nn.Entropyloss used instead of NNLoss\n",
    "# 500 sample pics, low quality, 448 randomcrop frame size\n",
    "train_test(epochs, model, train_loader, device, optimizer, criterion, print_every, valid_loader)\n",
    "# Result: 95.3s\n",
    "# 28% acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 116.00 MiB (GPU 0; 11.17 GiB total capacity; 10.45 GiB already allocated; 93.31 MiB free; 10.69 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-354e049c1915>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 1000 sample pics, medium quality, 224 randomcrop frame size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Result: 534.9s, more than 10x longer than small picture. There are ~10,000 pictures we will need to train the model on eventually, implying that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# it will take ~5,349s = ~1.5hrs to train the model on the entire dataset with current parameters, which sounds fair.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# The low validation accuracy has me worried regarding how the loss criterion is being calculated.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-b79956941790>\u001b[0m in \u001b[0;36mtrain_test\u001b[0;34m(epochs, model, train_loader, device, optimizer, criterion, print_every, valid_loader)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/efficientnet_pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# Convolution layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;31m# Pooling and final linear layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/efficientnet_pytorch/model.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# Stem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bn0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_stem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;31m# Blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/efficientnet_pytorch/utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/modules/padding.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'constant'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_pad\u001b[0;34m(input, pad, mode, value)\u001b[0m\n\u001b[1;32m   3550\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Padding length too large'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'constant'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3552\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_pad_nd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3553\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3554\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Padding mode \"{}\"\" doesn\\'t take in value argument'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 116.00 MiB (GPU 0; 11.17 GiB total capacity; 10.45 GiB already allocated; 93.31 MiB free; 10.69 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# 1000 sample pics, medium quality, 224 randomcrop frame size\n",
    "train_test(epochs, model, train_loader, device, optimizer, criterion, print_every, valid_loader)\n",
    "# Result: 534.9s, more than 10x longer than small picture. There are ~10,000 pictures we will need to train the model on eventually, implying that\n",
    "# it will take ~53,490s = ~15hrs to train the model on the entire dataset with current parameters, which sounds a bit brutal, especially because\n",
    "# I'm paying for an AWS instance that costs $1.2/hr\n",
    "# The low validation accuracy has me worried regarding how the loss criterion is being calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10..| Train loss: -0.057..| Validation loss: -0.013..| Validation accuracy: 0.333|\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-9222a1c6fcb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 1000 sample pics, medium quality, 448 randomcrop frame size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Result: 52.3s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-b79956941790>\u001b[0m in \u001b[0;36mtrain_test\u001b[0;34m(epochs, model, train_loader, device, optimizer, criterion, print_every, valid_loader)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-f57323aee211>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     33\u001b[0m                                                                    [0.229, 0.224, 0.225])])\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \"\"\"\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mnpimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'F'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mpic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mnpimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1000 sample pics, medium quality, 448 randomcrop frame size\n",
    "train_test(epochs, model, train_loader, device, optimizer, criterion, print_every, valid_loader)\n",
    "# Result: 52.3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10..| Train loss: 1.817..| Validation loss: 0.664..| Validation accuracy: 0.250|\n",
      "Epoch 2/10..| Train loss: 0.628..| Validation loss: 0.736..| Validation accuracy: 0.150|\n",
      "Epoch 2/10..| Train loss: 1.523..| Validation loss: 0.673..| Validation accuracy: 0.350|\n",
      "Epoch 3/10..| Train loss: 1.299..| Validation loss: 0.708..| Validation accuracy: 0.250|\n",
      "Epoch 4/10..| Train loss: 0.286..| Validation loss: 0.756..| Validation accuracy: 0.300|\n",
      "Epoch 4/10..| Train loss: 1.328..| Validation loss: 0.739..| Validation accuracy: 0.300|\n",
      "Epoch 5/10..| Train loss: 0.781..| Validation loss: 0.857..| Validation accuracy: 0.100|\n",
      "Epoch 5/10..| Train loss: 1.255..| Validation loss: 1.065..| Validation accuracy: 0.200|\n",
      "Epoch 6/10..| Train loss: 1.396..| Validation loss: 0.775..| Validation accuracy: 0.150|\n",
      "Epoch 7/10..| Train loss: 0.423..| Validation loss: 0.912..| Validation accuracy: 0.100|\n",
      "Epoch 7/10..| Train loss: 1.325..| Validation loss: 1.090..| Validation accuracy: 0.200|\n",
      "Epoch 8/10..| Train loss: 0.854..| Validation loss: 0.956..| Validation accuracy: 0.300|\n",
      "Epoch 9/10..| Train loss: 0.278..| Validation loss: 0.767..| Validation accuracy: 0.300|\n",
      "Epoch 9/10..| Train loss: 1.127..| Validation loss: 1.358..| Validation accuracy: 0.050|\n",
      "Epoch 10/10..| Train loss: 0.725..| Validation loss: 1.324..| Validation accuracy: 0.200|\n",
      "Epoch 10/10..| Train loss: 1.377..| Validation loss: 0.918..| Validation accuracy: 0.350|\n",
      "685.7173616886139\n"
     ]
    }
   ],
   "source": [
    "# 1000 sample pics, medium quality, 448 randomcrop frame size\n",
    "train_test(epochs, model, train_loader, device, optimizer, criterion, print_every, valid_loader)\n",
    "# Result: 52.3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10..| Train loss: 1.682..| Validation loss: 14.017..| Validation accuracy: 0.282|\n",
      "Epoch 1/10..| Train loss: 1.806..| Validation loss: 13.144..| Validation accuracy: 0.250|\n",
      "Epoch 1/10..| Train loss: 1.729..| Validation loss: 12.920..| Validation accuracy: 0.271|\n",
      "Epoch 1/10..| Train loss: 1.629..| Validation loss: 12.757..| Validation accuracy: 0.292|\n",
      "Epoch 1/10..| Train loss: 1.662..| Validation loss: 13.192..| Validation accuracy: 0.267|\n",
      "Epoch 1/10..| Train loss: 1.686..| Validation loss: 13.192..| Validation accuracy: 0.267|\n",
      "Epoch 1/10..| Train loss: 1.594..| Validation loss: 13.007..| Validation accuracy: 0.282|\n",
      "Epoch 1/10..| Train loss: 1.638..| Validation loss: 12.663..| Validation accuracy: 0.303|\n",
      "Epoch 1/10..| Train loss: 1.701..| Validation loss: 12.768..| Validation accuracy: 0.234|\n",
      "Epoch 1/10..| Train loss: 1.684..| Validation loss: 12.400..| Validation accuracy: 0.272|\n",
      "Epoch 1/10..| Train loss: 1.535..| Validation loss: 17.165..| Validation accuracy: 0.270|\n",
      "Epoch 1/10..| Train loss: 1.597..| Validation loss: 13.331..| Validation accuracy: 0.322|\n",
      "Epoch 1/10..| Train loss: 1.615..| Validation loss: 16.175..| Validation accuracy: 0.277|\n",
      "Epoch 1/10..| Train loss: 1.586..| Validation loss: 12.782..| Validation accuracy: 0.269|\n",
      "Epoch 1/10..| Train loss: 1.589..| Validation loss: 13.010..| Validation accuracy: 0.268|\n",
      "Epoch 1/10..| Train loss: 1.762..| Validation loss: 13.074..| Validation accuracy: 0.267|\n",
      "Epoch 1/10..| Train loss: 1.623..| Validation loss: 13.246..| Validation accuracy: 0.243|\n",
      "Epoch 1/10..| Train loss: 1.613..| Validation loss: 12.781..| Validation accuracy: 0.289|\n",
      "Epoch 1/10..| Train loss: 1.553..| Validation loss: 12.127..| Validation accuracy: 0.319|\n",
      "Epoch 1/10..| Train loss: 1.645..| Validation loss: 17.351..| Validation accuracy: 0.155|\n",
      "Epoch 1/10..| Train loss: 1.615..| Validation loss: 12.512..| Validation accuracy: 0.301|\n",
      "Epoch 1/10..| Train loss: 1.602..| Validation loss: 13.316..| Validation accuracy: 0.326|\n",
      "Epoch 1/10..| Train loss: 1.627..| Validation loss: 12.354..| Validation accuracy: 0.309|\n",
      "Epoch 1/10..| Train loss: 1.567..| Validation loss: 13.758..| Validation accuracy: 0.263|\n",
      "Epoch 1/10..| Train loss: 1.618..| Validation loss: 12.272..| Validation accuracy: 0.309|\n",
      "Epoch 1/10..| Train loss: 1.618..| Validation loss: 12.138..| Validation accuracy: 0.341|\n",
      "Epoch 1/10..| Train loss: 1.651..| Validation loss: 23.495..| Validation accuracy: 0.156|\n",
      "Epoch 1/10..| Train loss: 1.547..| Validation loss: 12.454..| Validation accuracy: 0.380|\n",
      "Epoch 1/10..| Train loss: 1.568..| Validation loss: 13.573..| Validation accuracy: 0.311|\n",
      "Epoch 1/10..| Train loss: 1.593..| Validation loss: 13.448..| Validation accuracy: 0.330|\n",
      "Epoch 1/10..| Train loss: 1.770..| Validation loss: 12.204..| Validation accuracy: 0.327|\n",
      "Epoch 1/10..| Train loss: 1.564..| Validation loss: 12.846..| Validation accuracy: 0.339|\n",
      "Epoch 1/10..| Train loss: 1.451..| Validation loss: 14.144..| Validation accuracy: 0.321|\n",
      "Epoch 1/10..| Train loss: 1.620..| Validation loss: 11.804..| Validation accuracy: 0.351|\n",
      "Epoch 1/10..| Train loss: 1.575..| Validation loss: 12.578..| Validation accuracy: 0.343|\n",
      "Epoch 1/10..| Train loss: 1.544..| Validation loss: 13.328..| Validation accuracy: 0.255|\n"
     ]
    }
   ],
   "source": [
    "# 10,000 sample pics, medium quality, 448 randomcrop frame size\n",
    "train_test(epochs, model, train_loader, device, optimizer, criterion, print_every, valid_loader)\n",
    "# Result: 52.3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 S ec2-user  7222  7108  0  80   0 - 281343 SyS_ep 18:14 ?       00:00:23 /home/ec2-user/anaconda3/envs/JupyterSystemEnv/bin/python /home/ec2-user/anaconda3/envs/JupyterSystemEnv/bin/jupyter-notebook --notebook-dir=/home/ec2-user/SageMaker/ --ip=0.0.0.0 --NotebookApp.token= --port=8443 --NotebookApp.disable_check_xsrf=True --certfile=/home/ec2-user/.jupyter/notebookcert.pem --keyfile /home/ec2-user/.jupyter/notebookkey.key\n",
      "4 S 502       7258  7257  0  80   0 - 224085 -     18:14 ?        00:00:00 /home/nbiagent-user/miniconda/bin/python /home/nbiagent-user/miniconda/bin/nbiagent /home/nbiagent-user/nbiagentcert.pem /home/nbiagent-user/nbiagentkey.key\n",
      "0 S ec2-user 10045  7222  2  80   0 - 21989392 core_s 18:24 ?     00:00:46 /home/ec2-user/anaconda3/envs/python3/bin/python -m ipykernel -f /home/ec2-user/.local/share/jupyter/runtime/kernel-880437f3-bc31-4dbb-8531-8a4f887b0c46.json\n",
      "0 S ec2-user 18100 10045  0  80   0 - 28289 -      19:01 pts/1    00:00:00 /bin/sh -c ps -elf | grep python\n",
      "0 S ec2-user 18102 18100  0  80   0 - 27633 -      19:01 pts/1    00:00:00 grep python\n"
     ]
    }
   ],
   "source": [
    "!ps -elf | grep python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11996954624"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_properties(device).total_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting GPUtil\n",
      "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
      "Building wheels for collected packages: GPUtil\n",
      "  Building wheel for GPUtil (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=37d01e19df60684b03270e663dc45330f583a21767e69052ff4b75e1e8edc05e\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/79/c1/b2/b6fc2647f693a084da25e1d31328ab3dbb565cc58fea37e973\n",
      "Successfully built GPUtil\n",
      "Installing collected packages: GPUtil\n",
      "Successfully installed GPUtil-1.4.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: -c: line 1: syntax error: unexpected end of file\n"
     ]
    }
   ],
   "source": [
    "!cuda.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 19% |\n"
     ]
    }
   ],
   "source": [
    "import GPUtil\n",
    "GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 17 01:29:02 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   42C    P0    79W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
